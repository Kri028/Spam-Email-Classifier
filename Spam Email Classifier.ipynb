{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64efba7e-f7a9-4568-9285-23f07e5a4283",
   "metadata": {},
   "source": [
    " # Spam classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9336b5-a82c-4edb-8a7e-f48cec045f66",
   "metadata": {},
   "source": [
    "Download examples of spam and ham from Apache SpamAssassinâ€™s public datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "711c02bd-4db1-4e79-883d-b1796b675d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "import email\n",
    "import email.policy\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import re\n",
    "from html import unescape\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "cff2eb75-9e37-427c-a882-fe03186a9798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the root URL to download spam and ham datasets\n",
    "DOWNLOAD_ROOT = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
    "HAM_URL = DOWNLOAD_ROOT + \"20030228_easy_ham.tar.bz2\"\n",
    "SPAM_URL = DOWNLOAD_ROOT + \"20030228_spam.tar.bz2\"\n",
    "\n",
    "# Define the local directory path to store the downloaded datasets\n",
    "SPAM_PATH = os.path.join(\"datasets\", \"spam\")\n",
    "\n",
    "def fetch_spam_data(ham_url=HAM_URL, spam_url=SPAM_URL, spam_path=SPAM_PATH):\n",
    "    # Create the directory if it doesn't already exist\n",
    "    if not os.path.isdir(spam_path):\n",
    "        os.makedirs(spam_path)\n",
    "\n",
    "    # Download and extract both ham and spam files\n",
    "    for filename, url in ((\"ham.tar.bz2\", ham_url), (\"spam.tar.bz2\", spam_url)):\n",
    "        path = os.path.join(spam_path, filename)\n",
    "\n",
    "        # Download the file only if it doesn't already exist\n",
    "        if not os.path.isfile(path):\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "\n",
    "        # Open the .tar.bz2 archive\n",
    "        tar_bz2_file = tarfile.open(path)\n",
    "\n",
    "        # Extract all contents into the spam_path directory\n",
    "        tar_bz2_file.extractall(path=spam_path)\n",
    "\n",
    "        # Close the archive file\n",
    "        tar_bz2_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "4dc1ca6a-ff3c-4215-b0ca-11c8e9f57c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r5/55rq7bkx1tg5zzbzq9whlzfh0000gn/T/ipykernel_60726/1779091595.py:26: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar_bz2_file.extractall(path=spam_path)\n"
     ]
    }
   ],
   "source": [
    "fetch_spam_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "a841dc8a-0341-4ac4-aa82-7daa4fa0985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the extracted \"ham\" (non-spam) email directory\n",
    "HAM_DIR = os.path.join(SPAM_PATH, \"easy_ham\")\n",
    "\n",
    "# Define the path to the extracted \"spam\" email directory\n",
    "SPAM_DIR = os.path.join(SPAM_PATH, \"spam\")\n",
    "\n",
    "# List all filenames in the ham directory, keeping only files with names longer than 20 characters\n",
    "# (to exclude system files or metadata like 'cmds')\n",
    "ham_filenames = [name for name in sorted(os.listdir(HAM_DIR)) if len(name) > 20]\n",
    "\n",
    "# List all filenames in the spam directory, also filtering out short/irrelevant files\n",
    "spam_filenames = [name for name in sorted(os.listdir(SPAM_DIR)) if len(name) > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "075f103b-df8c-4e83-abbb-1f3e501d869a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ham_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "5be06f0d-0f03-448c-bcbb-c5f07c1b2484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spam_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "f4b673f3-6b54-49ba-9b94-b2628b147e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_email(is_spam, filename, spam_path=SPAM_PATH):\n",
    "    # Choose the directory based on whether the email is spam or not\n",
    "    directory = \"spam\" if is_spam else \"easy_ham\"\n",
    "\n",
    "    # Construct the full path to the email file and open it in binary mode (\"rb\")\n",
    "    with open(os.path.join(spam_path, directory, filename), \"rb\") as f:\n",
    "        # Use BytesParser to parse the raw email bytes with a modern parsing policy\n",
    "        return email.parser.BytesParser(policy=email.policy.default).parse(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "f9bcb0d4-a6e0-4135-ba8d-b7e5d54a508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all ham (non-spam) emails using the load_email function\n",
    "ham_emails = [load_email(is_spam=False, filename=name) for name in ham_filenames]\n",
    "\n",
    "# Load all spam emails using the load_email function\n",
    "spam_emails = [load_email(is_spam=True, filename=name) for name in spam_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "14d1b6de-370f-47a7-875b-92c1825db1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Man Threatens Explosion In Moscow \n",
      "\n",
      "Thursday August 22, 2002 1:40 PM\n",
      "MOSCOW (AP) - Security officers on Thursday seized an unidentified man who\n",
      "said he was armed with explosives and threatened to blow up his truck in\n",
      "front of Russia's Federal Security Services headquarters in Moscow, NTV\n",
      "television reported.\n",
      "The officers seized an automatic rifle the man was carrying, then the man\n",
      "got out of the truck and was taken into custody, NTV said. No other details\n",
      "were immediately available.\n",
      "The man had demanded talks with high government officials, the Interfax and\n",
      "ITAR-Tass news agencies said. Ekho Moskvy radio reported that he wanted to\n",
      "talk with Russian President Vladimir Putin.\n",
      "Police and security forces rushed to the Security Service building, within\n",
      "blocks of the Kremlin, Red Square and the Bolshoi Ballet, and surrounded the\n",
      "man, who claimed to have one and a half tons of explosives, the news\n",
      "agencies said. Negotiations continued for about one and a half hours outside\n",
      "the building, ITAR-Tass and Interfax reported, citing witnesses.\n",
      "The man later drove away from the building, under police escort, and drove\n",
      "to a street near Moscow's Olympic Penta Hotel, where authorities held\n",
      "further negotiations with him, the Moscow police press service said. The\n",
      "move appeared to be an attempt by security services to get him to a more\n",
      "secure location. \n",
      "\n",
      "------------------------ Yahoo! Groups Sponsor ---------------------~-->\n",
      "4 DVDs Free +s&p Join Now\n",
      "http://us.click.yahoo.com/pt6YBB/NXiEAA/mG3HAA/7gSolB/TM\n",
      "---------------------------------------------------------------------~->\n",
      "\n",
      "To unsubscribe from this group, send an email to:\n",
      "forteana-unsubscribe@egroups.com\n",
      "\n",
      " \n",
      "\n",
      "Your use of Yahoo! Groups is subject to http://docs.yahoo.com/info/terms/\n"
     ]
    }
   ],
   "source": [
    "print(ham_emails[2].get_content().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "b8314495-11c5-4853-9663-9ab826e3776b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I thought you might like these:\n",
      "1) Slim Down - Guaranteed to lose 10-12 lbs in 30 days\n",
      "http://www.freeyankee.com/cgi/fy2/to.cgi?l=822slim1\n",
      "\n",
      "2) Fight The Risk of Cancer! \n",
      "http://www.freeyankee.com/cgi/fy2/to.cgi?l=822nic1 \n",
      "\n",
      "3) Get the Child Support You Deserve - Free Legal Advice \n",
      "http://www.freeyankee.com/cgi/fy2/to.cgi?l=822ppl1\n",
      "\n",
      "Offer Manager\n",
      "Daily-Deals\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "If you wish to leave this list please use the link below.\n",
      "http://www.qves.com/trim/?social@linux.ie%7C29%7C134077\n",
      "\n",
      "\n",
      "-- \n",
      "Irish Linux Users' Group Social Events: social@linux.ie\n",
      "http://www.linux.ie/mailman/listinfo/social for (un)subscription information.\n",
      "List maintainer: listmaster@linux.ie\n"
     ]
    }
   ],
   "source": [
    "print(spam_emails[4].get_content().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "c333f11e-2494-4c37-98ef-df7d5d554a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_structure(email):\n",
    "    # If the input is already a plain string (not an email object), return it as-is\n",
    "    if isinstance(email, str):\n",
    "        return email\n",
    "\n",
    "    # Get the payload (content) of the email\n",
    "    payload = email.get_payload()\n",
    "\n",
    "    # If the payload is a list, it's a multipart email (e.g., contains both plain text and HTML parts)\n",
    "    if isinstance(payload, list):\n",
    "        # Recursively call get_email_structure on each part and join their structures\n",
    "        return \"multipart({})\".format(\", \".join([\n",
    "            get_email_structure(sub_email)\n",
    "            for sub_email in payload\n",
    "        ]))\n",
    "    else:\n",
    "        # If it's a single part, return its content type (e.g., text/plain, text/html)\n",
    "        return email.get_content_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "1ae1f9b4-eb10-4e94-9dd2-0fef949e2d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structures_counter(emails):\n",
    "    # Initialize a counter to count occurrences of each email structure\n",
    "    structures = Counter()\n",
    "    \n",
    "    # Iterate through each email in the provided list\n",
    "    for email in emails:\n",
    "        # Get the structure of the email (e.g., text/plain, multipart(...))\n",
    "        structure = get_email_structure(email)\n",
    "        \n",
    "        # Increment the count for this structure\n",
    "        structures[structure] += 1\n",
    "\n",
    "    # Return the dictionary-like Counter object showing frequencies of each structure\n",
    "    return structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "cd058ef2-7a61-452f-96ab-846c6c57be37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 2408),\n",
       " ('multipart(text/plain, application/pgp-signature)', 66),\n",
       " ('multipart(text/plain, text/html)', 8),\n",
       " ('multipart(text/plain, text/plain)', 4),\n",
       " ('multipart(text/plain)', 3),\n",
       " ('multipart(text/plain, application/octet-stream)', 2),\n",
       " ('multipart(text/plain, text/enriched)', 1),\n",
       " ('multipart(text/plain, application/ms-tnef, text/plain)', 1),\n",
       " ('multipart(multipart(text/plain, text/plain, text/plain), application/pgp-signature)',\n",
       "  1),\n",
       " ('multipart(text/plain, video/mng)', 1),\n",
       " ('multipart(text/plain, multipart(text/plain))', 1),\n",
       " ('multipart(text/plain, application/x-pkcs7-signature)', 1),\n",
       " ('multipart(text/plain, multipart(text/plain, text/plain), text/rfc822-headers)',\n",
       "  1),\n",
       " ('multipart(text/plain, multipart(text/plain, text/plain), multipart(multipart(text/plain, application/x-pkcs7-signature)))',\n",
       "  1),\n",
       " ('multipart(text/plain, application/x-java-applet)', 1)]"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(ham_emails).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "39b46da8-5365-4d56-9269-2707f2fd7e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 218),\n",
       " ('text/html', 183),\n",
       " ('multipart(text/plain, text/html)', 45),\n",
       " ('multipart(text/html)', 20),\n",
       " ('multipart(text/plain)', 19),\n",
       " ('multipart(multipart(text/html))', 5),\n",
       " ('multipart(text/plain, image/jpeg)', 3),\n",
       " ('multipart(text/html, application/octet-stream)', 2),\n",
       " ('multipart(text/plain, application/octet-stream)', 1),\n",
       " ('multipart(text/html, text/plain)', 1),\n",
       " ('multipart(multipart(text/html), application/octet-stream, image/jpeg)', 1),\n",
       " ('multipart(multipart(text/plain, text/html), image/gif)', 1),\n",
       " ('multipart/alternative', 1)]"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(spam_emails).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "fe8db494-ab7d-4f96-9e3a-0f51ac505577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return-Path : <12a1mailbot1@web.de>\n",
      "Delivered-To : zzzz@localhost.spamassassin.taint.org\n",
      "Received : from localhost (localhost [127.0.0.1])\tby phobos.labs.spamassassin.taint.org (Postfix) with ESMTP id 136B943C32\tfor <zzzz@localhost>; Thu, 22 Aug 2002 08:17:21 -0400 (EDT)\n",
      "Received : from mail.webnote.net [193.120.211.219]\tby localhost with POP3 (fetchmail-5.9.0)\tfor zzzz@localhost (single-drop); Thu, 22 Aug 2002 13:17:21 +0100 (IST)\n",
      "Received : from dd_it7 ([210.97.77.167])\tby webnote.net (8.9.3/8.9.3) with ESMTP id NAA04623\tfor <zzzz@spamassassin.taint.org>; Thu, 22 Aug 2002 13:09:41 +0100\n",
      "From : 12a1mailbot1@web.de\n",
      "Received : from r-smtp.korea.com - 203.122.2.197 by dd_it7  with Microsoft SMTPSVC(5.5.1775.675.6);\t Sat, 24 Aug 2002 09:42:10 +0900\n",
      "To : dcek1a1@netsgo.com\n",
      "Subject : Life Insurance - Why Pay More?\n",
      "Date : Wed, 21 Aug 2002 20:31:57 -1600\n",
      "MIME-Version : 1.0\n",
      "Message-ID : <0103c1042001882DD_IT7@dd_it7>\n",
      "Content-Type : text/html; charset=\"iso-8859-1\"\n",
      "Content-Transfer-Encoding : quoted-printable\n"
     ]
    }
   ],
   "source": [
    "for header, value in spam_emails[0].items():\n",
    "    print(header, \":\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "58cf10fc-f5f5-4eb4-9566-263ed467c8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[ILUG] Guaranteed to lose 10-12 lbs in 30 days 10.206'"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_emails[1][\"Subject\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "864695ec-fbfe-45c2-9dd1-a29daab30114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine ham and spam emails into a single NumPy array (X)\n",
    "# Using dtype=object because the elements are complex objects (email messages)\n",
    "X = np.array(ham_emails + spam_emails, dtype=object)\n",
    "\n",
    "# Create labels: 0 for ham (non-spam), 1 for spam\n",
    "y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "# 80% training data, 20% testing data\n",
    "# random_state=42 ensures reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "7efc543c-d509-4197-8691-506a8954d585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_plain_text(html):\n",
    "    text = re.sub('<head.*?>.*?</head>', '', html, flags=re.M | re.S | re.I)\n",
    "    text = re.sub('<a\\s.*?>',   'HYPERLINK', text, flags=re.M | re.S | re.I)\n",
    "    text = re.sub('<.*?>', '', text, flags=re.M | re.S)\n",
    "    text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\n",
    "    return unescape(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "6b9f16d0-1b04-4fea-8b1e-92cd0f78e8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def html_to_plain_text(html):\n",
    "    # Remove the content inside <head>...</head> tags (e.g., metadata, scripts, styles)\n",
    "    text = re.sub('<head.*?>.*?</head>', '', html, flags=re.M | re.S | re.I)\n",
    "\n",
    "    # Replace all hyperlinks with a placeholder text \"HYPERLINK\"\n",
    "    text = re.sub('<a\\s.*?>', 'HYPERLINK', text, flags=re.M | re.S | re.I)\n",
    "\n",
    "    # Remove all remaining HTML tags (e.g., <div>, <p>, <br>, etc.)\n",
    "    text = re.sub('<.*?>', '', text, flags=re.M | re.S)\n",
    "\n",
    "    # Replace multiple consecutive newlines and whitespace with a single newline\n",
    "    text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\n",
    "\n",
    "    # Convert HTML entities (like &amp;, &gt;) to their corresponding characters\n",
    "    return unescape(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "5f1f4d5c-ff46-4284-89e9-e8ed1b264fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OTC\n",
      "Â Newsletter\n",
      "Discover Tomorrow's WinnersÂ \n",
      "For Immediate Release\n",
      "Cal-Bay (Stock Symbol: CBYI)\n",
      "Watch for analyst \"Strong Buy Recommendations\" and several advisory newsletters picking CBYI.  CBYI has filed to be traded on the OTCBB, share prices historically INCREASE when companies get listed on this larger trading exchange. CBYI is trading around 25 cents and should skyrocket to $2.66 - $3.25 a share in the near future.\n",
      "Put CBYI on your watch list, acquire a position TODAY.\n",
      "REASONS TO INVEST IN CBYI\n",
      "A profitable company and is on track to beat ALL earnings estimates!\n",
      "One of the FASTEST growing distributors in environmental & safety equipment instruments.\n",
      "Excellent management team, several EXCLUSIVE contracts.  IMPRESSIVE client list including the U.S. Air Force, Anheuser-Busch, Chevron Refining and Mitsubishi Heavy Industries, GE-Energy & Environmental Research.\n",
      "RAPIDLY GROWING INDUSTRY\n",
      "Industry revenues exceed $900 million, estimates indicate that there could be as much as $25 billi ...\n"
     ]
    }
   ],
   "source": [
    "print(html_to_plain_text(sample_html_spam.get_content())[:1000], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "df126e78-5162-43aa-aed9-afecfbd67325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_to_text(email):\n",
    "    html = None  # Initialize a variable to store HTML content if found\n",
    "\n",
    "    # Traverse all parts of the email (especially useful for multipart emails)\n",
    "    for part in email.walk():\n",
    "        ctype = part.get_content_type()  # Get the MIME type (e.g., text/plain, text/html)\n",
    "\n",
    "        # Skip parts that are not plain text or HTML\n",
    "        if ctype not in (\"text/plain\", \"text/html\"):\n",
    "            continue\n",
    "\n",
    "        # Try to get the email content safely (handle decoding issues)\n",
    "        try:\n",
    "            content = part.get_content()\n",
    "        except:\n",
    "            # Fallback in case decoding fails\n",
    "            content = str(part.get_payload())\n",
    "\n",
    "        # If plain text is found, return it directly\n",
    "        if ctype == \"text/plain\":\n",
    "            return content\n",
    "        else:\n",
    "            # Save HTML content in case no plain text is found\n",
    "            html = content\n",
    "\n",
    "    # If no plain text was found, convert the HTML content to plain text\n",
    "    if html:\n",
    "        return html_to_plain_text(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "115a4cb1-6c98-4a9a-b3df-7d0b2241ee6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OTC\n",
      "Â Newsletter\n",
      "Discover Tomorrow's WinnersÂ \n",
      "For Immediate Release\n",
      "Cal-Bay (Stock Symbol: CBYI)\n",
      "Wat ...\n"
     ]
    }
   ],
   "source": [
    "print(email_to_text(sample_html_spam)[:100], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "9a71f1a7-e35f-482e-97fd-0c232d195205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations => comput\n",
      "Computation => comput\n",
      "Computing => comput\n",
      "Computed => comput\n",
      "Compute => comput\n",
      "Compulsive => compuls\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import nltk\n",
    "\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    for word in (\"Computations\", \"Computation\", \"Computing\", \"Computed\", \"Compute\", \"Compulsive\"):\n",
    "        print(word, \"=>\", stemmer.stem(word))\n",
    "except ImportError:\n",
    "    print(\"Error: stemming requires the NLTK modulle.\")\n",
    "    stemmer= None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "cf95b479-f5fe-4015-ba7f-f8f1ad18124c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['github.com', 'https://youtu.be/7Pq-S557XQU?t=3m32s']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Try importing the URLExtract class from the urlextract module\n",
    "    import urlextract\n",
    "\n",
    "    # Create an instance of the URL extractor\n",
    "    url_extractor = urlextract.URLExtract()\n",
    "\n",
    "    # Test the extractor on a sample string with URLs\n",
    "    print(url_extractor.find_urls(\"Will it detect github.com and https://youtu.be/7Pq-S557XQU?t=3m32s\"))\n",
    "\n",
    "except ImportError:\n",
    "    # If the urlextract module is not installed, show a warning and set url_extractor to None\n",
    "    print(\"Error: replacing URLs requires the urlextract module.\")\n",
    "    url_extractor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "61c18979-4b00-4bb7-988f-357ba7711c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strip_headers=True, lower_case=True, remove_punctuation=True,\n",
    "                 replace_urls=True, replace_numbers=True, stemming=True):\n",
    "        # Configurable preprocessing options\n",
    "        self.strip_headers = strip_headers            # (Not used in transform, could be added)\n",
    "        self.lower_case = lower_case                    # Convert text to lowercase\n",
    "        self.remove_punctuation = remove_punctuation    # Remove punctuation characters\n",
    "        self.replace_urls = replace_urls                # Replace URLs with \"URL\" token\n",
    "        self.replace_numbers = replace_numbers          # Replace numbers with \"NUMBER\" token\n",
    "        self.stemming = stemming                          # Apply stemming to reduce words to roots\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # No training needed; just return self\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "\n",
    "        # Process each email in the input dataset X\n",
    "        for email in X:\n",
    "            # Extract the email text (plain or html cleaned)\n",
    "            text = email_to_text(email) or \"\"\n",
    "\n",
    "            # Convert to lowercase if option enabled\n",
    "            if self.lower_case:\n",
    "                text = text.lower()\n",
    "\n",
    "            # Replace URLs if option enabled and url_extractor is available\n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                # Extract unique URLs found in the text\n",
    "                urls = list(set(url_extractor.find_urls(text)))\n",
    "\n",
    "                # Sort URLs by length descending to avoid partial replacements\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "\n",
    "                # Replace each URL in the text with the placeholder \"URL\"\n",
    "                for url in urls:\n",
    "                    text = text.replace(url, \" URL \")\n",
    "\n",
    "            # Replace numbers (including floats and scientific notation) with \"NUMBER\"\n",
    "            if self.replace_numbers:\n",
    "                text = re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?', 'NUMBER', text)\n",
    "\n",
    "            # Remove punctuation by replacing non-word characters with spaces\n",
    "            if self.remove_punctuation:\n",
    "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
    "\n",
    "            # Count the frequency of each word in the cleaned text\n",
    "            word_counts = Counter(text.split())\n",
    "\n",
    "            # Apply stemming if enabled and stemmer is available\n",
    "            if self.stemming and stemmer is not None:\n",
    "                stemmed_word_counts = Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    stemmed_word = stemmer.stem(word)\n",
    "                    stemmed_word_counts[stemmed_word] += count\n",
    "                word_counts = stemmed_word_counts\n",
    "\n",
    "            # Append the word frequency dictionary for this email\n",
    "            X_transformed.append(word_counts)\n",
    "\n",
    "        # Return a numpy array of word count dictionaries (dtype=object)\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "16acad86-4e8d-4c5c-866f-6ca5721d7be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Counter({'chuck': 1, 'murcko': 1, 'wrote': 1, 'stuff': 1, 'yawn': 1, 'r': 1}),\n",
       "       Counter({'the': 11, 'of': 9, 'and': 8, 'all': 3, 'christian': 3, 'to': 3, 'by': 3, 'jefferson': 2, 'i': 2, 'have': 2, 'superstit': 2, 'one': 2, 'on': 2, 'been': 2, 'ha': 2, 'half': 2, 'rogueri': 2, 'teach': 2, 'jesu': 2, 'some': 1, 'interest': 1, 'quot': 1, 'url': 1, 'thoma': 1, 'examin': 1, 'known': 1, 'word': 1, 'do': 1, 'not': 1, 'find': 1, 'in': 1, 'our': 1, 'particular': 1, 'redeem': 1, 'featur': 1, 'they': 1, 'are': 1, 'alik': 1, 'found': 1, 'fabl': 1, 'mytholog': 1, 'million': 1, 'innoc': 1, 'men': 1, 'women': 1, 'children': 1, 'sinc': 1, 'introduct': 1, 'burnt': 1, 'tortur': 1, 'fine': 1, 'imprison': 1, 'what': 1, 'effect': 1, 'thi': 1, 'coercion': 1, 'make': 1, 'world': 1, 'fool': 1, 'other': 1, 'hypocrit': 1, 'support': 1, 'error': 1, 'over': 1, 'earth': 1, 'six': 1, 'histor': 1, 'american': 1, 'john': 1, 'e': 1, 'remsburg': 1, 'letter': 1, 'william': 1, 'short': 1, 'again': 1, 'becom': 1, 'most': 1, 'pervert': 1, 'system': 1, 'that': 1, 'ever': 1, 'shone': 1, 'man': 1, 'absurd': 1, 'untruth': 1, 'were': 1, 'perpetr': 1, 'upon': 1, 'a': 1, 'larg': 1, 'band': 1, 'dupe': 1, 'import': 1, 'led': 1, 'paul': 1, 'first': 1, 'great': 1, 'corrupt': 1}),\n",
       "       Counter({'url': 4, 's': 3, 'group': 3, 'to': 3, 'in': 2, 'forteana': 2, 'martin': 2, 'an': 2, 'and': 2, 'we': 2, 'is': 2, 'yahoo': 2, 'unsubscrib': 2, 'y': 1, 'adamson': 1, 'wrote': 1, 'for': 1, 'altern': 1, 'rather': 1, 'more': 1, 'factual': 1, 'base': 1, 'rundown': 1, 'on': 1, 'hamza': 1, 'career': 1, 'includ': 1, 'hi': 1, 'belief': 1, 'that': 1, 'all': 1, 'non': 1, 'muslim': 1, 'yemen': 1, 'should': 1, 'be': 1, 'murder': 1, 'outright': 1, 'know': 1, 'how': 1, 'unbias': 1, 'memri': 1, 'don': 1, 't': 1, 'html': 1, 'rob': 1, 'sponsor': 1, 'number': 1, 'dvd': 1, 'free': 1, 'p': 1, 'join': 1, 'now': 1, 'from': 1, 'thi': 1, 'send': 1, 'email': 1, 'egroup': 1, 'com': 1, 'your': 1, 'use': 1, 'of': 1, 'subject': 1})],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few = X_train[:3]  # Take the first 3 emails from the training set\n",
    "X_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)  # Transform those emails into word count vectors\n",
    "X_few_wordcounts  # Display the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "744e5911-598b-45db-8e82-3cdad37752c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        # Max number of words to keep in vocabulary (most frequent)\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Aggregate word counts across all documents (emails)\n",
    "        total_count = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                # Clip count to 10 to reduce effect of very frequent words\n",
    "                total_count[word] += min(count, 10)\n",
    "\n",
    "        # Select the most common words up to the vocabulary size\n",
    "        most_common = total_count.most_common(self.vocabulary_size)\n",
    "\n",
    "        # Create a vocabulary mapping word -> index (starting from 1)\n",
    "        # Index 0 is reserved for unknown words (not in vocabulary)\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "\n",
    "        # Convert each word count dictionary into sparse matrix format\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                # Only include words found in the vocabulary, else index 0 (ignored by sparse matrix)\n",
    "                col_index = self.vocabulary_.get(word, 0)\n",
    "                rows.append(row)\n",
    "                cols.append(col_index)\n",
    "                data.append(count)\n",
    "\n",
    "        # Create a Compressed Sparse Row (CSR) matrix of shape (num_samples, vocabulary_size + 1)\n",
    "        # +1 because index 0 is for unknown words (will be zeroed out)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "00fbdcb7-7419-4229-9f9a-3d1fb84780a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 20 stored elements and shape (3, 11)>"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer = WordCounterToVectorTransformer(vocabulary_size=10)\n",
    "X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)\n",
    "X_few_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "fad68753-bd59-47c5-8c88-2620d79e54ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [99, 11,  9,  8,  3,  1,  3,  1,  3,  2,  3],\n",
       "       [67,  0,  1,  2,  3,  4,  1,  2,  0,  1,  0]])"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "8e13f16a-57b6-4b43-bad6-5ef27d44876f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'of': 2,\n",
       " 'and': 3,\n",
       " 'to': 4,\n",
       " 'url': 5,\n",
       " 'all': 6,\n",
       " 'in': 7,\n",
       " 'christian': 8,\n",
       " 'on': 9,\n",
       " 'by': 10}"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "234b2176-34af-424b-80d0-b31468a13cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline to preprocess raw email data into numeric feature vectors\n",
    "preprocess_pipeline = Pipeline([\n",
    "    # Step 1: Convert each email to a word frequency Counter dictionary\n",
    "    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n",
    "\n",
    "    # Step 2: Convert the word count dictionaries into sparse numeric vectors\n",
    "    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training emails and transform them into feature vectors\n",
    "X_train_transformed = preprocess_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "18fea7d7-9346-480b-9d76-47f05d429ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................................ score: (test=0.981) total time=   0.1s\n",
      "[CV] END ................................ score: (test=0.981) total time=   0.3s\n",
      "[CV] END ................................ score: (test=0.990) total time=   0.2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.9841666666666665)"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a Logistic Regression classifier with:\n",
    "# - 'lbfgs' solver (efficient for small/medium datasets)\n",
    "# - max_iter=1000 to allow enough iterations for convergence\n",
    "# - random_state=42 for reproducibility\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42)\n",
    "\n",
    "# Perform 3-fold cross-validation on the training data\n",
    "# Evaluates the model's performance by splitting data into 3 parts and training/testing\n",
    "# verbose=3 shows detailed progress output during cross-validation\n",
    "score = cross_val_score(log_clf, X_train_transformed, y_train, cv=3, verbose=3)\n",
    "\n",
    "# Calculate and return the average accuracy score across the 3 folds\n",
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "5408d7ba-a827-4517-a2d2-53f98c17485b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 94.90%\n",
      "Recall: 97.89%\n"
     ]
    }
   ],
   "source": [
    "# Transform the test emails into feature vectors using the same preprocessing pipeline\n",
    "X_test_transformed = preprocess_pipeline.transform(X_test)\n",
    "\n",
    "# Initialize the Logistic Regression classifier with the same parameters\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42)\n",
    "\n",
    "# Train the classifier on the full training dataset\n",
    "log_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict labels (spam=1, ham=0) for the test dataset\n",
    "y_pred = log_clf.predict(X_test_transformed)\n",
    "\n",
    "# Calculate and print the precision of the predictions\n",
    "# Precision = TP / (TP + FP), measures accuracy of positive (spam) predictions\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
    "\n",
    "# Calculate and print the recall of the predictions\n",
    "# Recall = TP / (TP + FN), measures ability to detect all positive (spam) cases\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
